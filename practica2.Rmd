------------------------------------------------------------------------

title: "Practica2" output: html_document date: "2025-01-13"

# Instalar y cargar las librerías necesarias

```{r}
if (!require(httr)) install.packages("httr")
if (!require(httr2)) install.packages("httr2")
if (!require(XML)) install.packages("XML")
if (!require(dplyr)) install.packages("dplyr")
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(gridExtra)) install.packages("gridExtra")
```

## Pregunta 1

```{r}
# Cargamos algunas librerías útiles
library(httr)
library(XML)
library(dplyr)
```

# Ejercicio 1

```{r}
# Cogemos la URL indicada, la descargamos y finalmente la pasamos a xml, ya que luego nos resultará más facil para analizar.
url <- "https://www.mediawiki.org/wiki/MediaWiki";

html <- GET(url)

xml <- htmlParse(html)
```

# Ejercicio 2

```{r}
# Usamos la siguiente función para buscar el título de la página y lo mostramos por pantalla.
title <- xpathSApply(xml, "//title", xmlValue)
show(title)
```

# Ejercicio 3

```{r}
# Buscamos en la página los enlaces y el texto con el que están escritos y lo guardamos en sus variables.
textos <- xpathSApply(xml, "//a", xmlValue)
links <- xpathSApply(xml, "//a", xmlGetAttr, "href")

# Buscamos los valores nulos que haya en la lista "links" y los sustituye por NA.
valors_nuls <- sapply(links, is.null)
links[valors_nuls] <- NA

# Recategorizamos la lista "links" por conveniencia.
links <- unlist(links)
```

# Ejercicio 4

```{r}
# Creamos el data frame con las URLs y sus respectivos textos.
taula_links <- data.frame( Textos = textos, Links = links)

# Creamos una nueva columna del data frame que nos dirá si la URL es absoluta (TRUE) o si no (FALSE).
taula_links$es_absoluta <- grepl("^http", taula_links$Links)

# Creamos una nueva columna del data frame para unir el dominio base a las URLs que sean relativas.
taula_links$full_link <- ifelse(taula_links$es_absoluta == FALSE, paste0("https://www.mediawiki.org", taula_links$Links), taula_links$Links )

# Creamos un data frame temporal para contar cuantas veces aparece cada enlace. Una vez hecho borramos esa data frame.
temp_taula <- taula_links %>% count(full_link)

taula_links <- left_join(taula_links, temp_taula, by = "full_link")
rm(temp_taula)

```

#Ejercicio 5

```{r}
# Empezamos creando una función que devuelve el código de status de una URL. A saber, 200 (petición correcta), 404 (no encontrado) o bien 403 (acceso restringido). Además hemos añadido un pequeño tiempo de espera entre cada petición para evitar ser baneados por el servidor.
status_link <- function(link) {
  Sys.sleep(0.2)
  response <- GET(link)
  status_code(response)
}

# Vectorizamos la anterior función para poder usarla en el mutate (siguiente linea).
status_link_vector <- Vectorize(status_link)

# Creamos una nueva columna de nuestro data frame en el que aparece el "status code" de la petición para cada URL.
taula_links <- taula_links %>%
  mutate(status = status_link_vector(taula_links$full_link))



```

## Pregunta 2

# Ejercicio 1

```{r}
# Creamos una nueva coluumna del data frame similar a una hecha anteriormente, pero con unas denominaciones distintas, puesto que nos ayudará a crear el gráfico que deseamos. Clasificamos las URLs por tipo: absolutas y relativas.
taula_links <- taula_links %>%
  mutate(type = ifelse(grepl("^http", Links), "Absoluta", "Relativa"))

# Agrupamos ahora por tipo y URLs; y contamos las frecuencias.
url_counts <- taula_links %>%
  group_by(type, Links) %>%
  summarise(count = n(), .groups = "drop")

# Agrupamos ahora por tipo y repeticiones de cada link.
aaa <- dplyr::count(url_counts, type, count)

# Creamos un gráfico de barras que representa la frecuencia con la que se repiten los enlaces y a la vez separándolo por "Absoluta" y "Relativa".
plot1 <- ggplot(aaa, aes(x = count, y = n, fill = type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "Frecuencia de URLs Absolutas y Relativas",
    x = "URLs",
    y = "Frecuencia",
    fill = "Tipo de URL"
  ) 
plot1

```
# Ejercicio 2
```{r}
# Creamos una nueva coluumna del data frame similar a una hecha anteriormente, pero con unas denominaciones distintas, puesto que nos ayudará a crear el gráfico que deseamos. Clasificamos las URLs no ser de mediawiki o serlo

taula_links <- taula_links %>%
  mutate(es_mediawiki = ifelse(grepl("^https://www.mediawiki.org", full_link), "True", "False"))


# Contamos cuantas URLs apuntan a otros dominios o servicios vs las que apuntan a https://www.mediawiki.org
conteo_mediawiki <- taula_links %>%
  group_by(es_mediawiki) %>%
  summarise(freq = n())

# Generamos el pie chart.
plot2 <- ggplot(conteo_mediawiki, aes(x = "", y = freq, fill = es_mediawiki)) +
  geom_bar(stat = "identity", position = "dodge") +
  #coord_polar("y") +
  labs(title = "Porcentaje de enlaces internos o externos", fill = "Es mediawiki") +
  scale_fill_manual(values = c("True" = "green", "False" = "red")) +
  theme_minimal()

plot2



```
# Ejercicio 3

```{r}
# Contamos cuantas URLs hay por cada código de status de la petición.
conteo_status <- taula_links %>%
  group_by(status) %>%
  summarise(freq = n())

# Cambiamos a tipo factor la columna "status" para que no nos la tome como contínua.
conteo_status$status <- factor(conteo_status$status)

# Generamos el pie chart.
plot3 <- ggplot(conteo_status, aes(x = "", y = freq, fill = status)) +
  geom_bar(stat = "identity") +
  coord_polar("y") +
  labs(title = "Porcentaje de status", fill = "Status code") +
  scale_fill_manual(values = c("200" = "blue", "404" = "green")) +
  theme_void()

plot3

```

```{r}
# Cargamos la libreria que nos permitirá combinar distintos gráficos en una sola figura.
library(gridExtra)

# Llamamos a la función y colocamos nuestros gráficos.
grid.arrange(plot1,         
  arrangeGrob(plot2, plot3, ncol = 2), nrow = 2)


```




